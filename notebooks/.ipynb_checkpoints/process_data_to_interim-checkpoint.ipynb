{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6397a610-1b78-4d3c-9697-9d5163d80c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test notebook\n",
    "# Here no format will be provided, it's just for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11d7bde-0e50-4a52-b11c-27f7064d06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "URL_TRIPADVISOR = 'https://www.tripadvisor.com'\n",
    "URL_EMPTY_STRING = 'https://www.this_is_an_empty_string.com'\n",
    "SECONDS_TO_DAYS = 1/60/60/24\n",
    "\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66d6483-61a0-42d5-92f1-df3c45a20ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_data_process(data):\n",
    "    df = data.copy() # We copy so that we don't manipulate the real data.\n",
    "    \n",
    "    # Let's sort values using the even time stamp. \n",
    "    df = df.sort_values('eventtimestamp')\n",
    "\n",
    "    df.loc[df.referrerurl=='', 'referrerurl'] = URL_EMPTY_STRING\n",
    "    \n",
    "    # Let's identify which links contain tripadvisor information\n",
    "    df.loc[df.targeturl.str.contains(URL_TRIPADVISOR), 'trip_advisor_presence'] = 1\n",
    "    df.trip_advisor_presence = df.trip_advisor_presence.fillna(0)\n",
    "    \n",
    "    # We are skipping those values that from a same url goes to the same url\n",
    "    df = df[~(\n",
    "        df.targeturl.str.contains(URL_TRIPADVISOR) & \n",
    "        df.referrerurl.str.contains(URL_TRIPADVISOR)\n",
    "    )]\n",
    "\n",
    "    # Now we care about the distinct chain of events that lead to each trip_advisor link\n",
    "    # We can consider those as \"sessions\" with the deffiniton that a session is a \"series\n",
    "    # of events that ultimately lead to a new \"TripAdvisor\" url link.\n",
    "    sessions = []\n",
    "    index_val_pos_keep = 0\n",
    "    for session_id, val in enumerate(df[df.trip_advisor_presence==1].index):\n",
    "        session_id = f\"_{session_id}\"\n",
    "        index_val_pos = df.index.get_loc(val) + 1\n",
    "        # We get the temporal dataframe from the index_val_pos_keep to index_val_pos\n",
    "        temporal_df = df.iloc[index_val_pos_keep : index_val_pos]\n",
    "        temporal_df['sessionid'] = temporal_df['userid'] + session_id\n",
    "        \n",
    "        if temporal_df.shape[0] == 1:\n",
    "            # Sometimes the session has just 1 link, if it was an empty string it\n",
    "            # doesn't help us to the analysis. Let's keep the ones that have something\n",
    "            # Different than an empty string.\n",
    "            temporal_df = temporal_df[~(\n",
    "                temporal_df.targeturl.str.contains(URL_TRIPADVISOR) & \n",
    "                temporal_df.referrerurl.str.contains(URL_EMPTY_STRING)\n",
    "            )]\n",
    "            \n",
    "        if temporal_df.shape[0] > 0:\n",
    "            sessions.append(temporal_df)\n",
    "        index_val_pos_keep = index_val_pos\n",
    "    # After iterations are done we actually removed what happened after the last TripAdvisor url\n",
    "    # We do not care of this. \n",
    "    if sessions:\n",
    "        # If there is a list of dataframes to concatenate, then we do the concat\n",
    "        df_sessions = pd.concat(sessions).drop('userid', axis=1)\n",
    "        # Depending on the lenght of the session will be relatively important to see if we should\n",
    "        # Keep it or not.\n",
    "        if df_sessions.shape[0] > 1:\n",
    "            return df_sessions\n",
    "        df_sessions['sessionid'] = 'something wrong'\n",
    "        return df_sessions\n",
    "    df['sessionid'] = 'something wrong'\n",
    "    df = df.drop('userid', axis=1)    \n",
    "    return df\n",
    "\n",
    "def identify_outliers(data):\n",
    "    df = data.copy()\n",
    "    df['click_seconds'] = (df.eventtimestamp - df.eventtimestamp.shift(1)).fillna(0)\n",
    "    df['click_days'] = df['click_seconds'] * SECONDS_TO_DAYS\n",
    "    \n",
    "    # Each user should have their own outlier behaviour\n",
    "    if df.shape[0]:\n",
    "        X = df['click_days']\n",
    "        try:\n",
    "            random_seed = 0\n",
    "            clf = IsolationForest(\n",
    "                random_state=random_seed, \n",
    "                n_estimators=100\n",
    "            ).fit(\n",
    "                X.dropna().sample(\n",
    "                    frac=0.1,\n",
    "                    random_state=random_seed)\n",
    "                .to_numpy().reshape(-1, 1) \n",
    "            )\n",
    "            y = clf.predict(X.fillna(0).to_numpy().reshape(-1,1))\n",
    "        except:\n",
    "            y = np.array([0]*df.shape[0])\n",
    "        df['outliers'] = y!=1\n",
    "        df['outliers'] = df['outliers'].astype('int')\n",
    "    else:\n",
    "        df['outliers'] = 0\n",
    "    cumsum_val = df.outliers.cumsum()\n",
    "    df['subsessionid'] = df.sessionid + '__' + cumsum_val.astype('str')\n",
    "    df['subsessionid_nb'] = cumsum_val.astype('int')\n",
    "    return df\n",
    "\n",
    "def get_grouped_subsessionid_list(data, col1='referrerurl', col2='targeturl'):\n",
    "    df = data.copy()\n",
    "    condition = df.click_seconds == 0 \n",
    "    condition.loc[condition.idxmax()] = False\n",
    "    df = df[~condition]\n",
    "    return df[[col1, col2]].to_numpy().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7142cb17-1da6-4c02-819d-12bc58705bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    data = pd.read_parquet(file_path)\n",
    "    test = data.groupby('userid').apply(first_data_process).reset_index()\n",
    "    \n",
    "    del data\n",
    "\n",
    "    # Let's not care about the 'something wrong': no trip_advisor_presence\n",
    "    test = test[test.sessionid!='something wrong'].reset_index(drop=True)\n",
    "    \n",
    "    url_parsed_referrer = test.referrerurl.apply(urlparse)\n",
    "    url_parsed_target = test.targeturl.apply(urlparse)\n",
    "\n",
    "    test['referrerurl_netloc'] = url_parsed_referrer.apply(lambda x: x.netloc)\n",
    "    test['referrerurl_query'] = url_parsed_referrer.apply(lambda x: x.query)\n",
    "    test['referrerurl_path'] = url_parsed_referrer.apply(lambda x: x.path)\n",
    "    test['targeturl_netloc'] = url_parsed_target.apply(lambda x: x.netloc)\n",
    "    test['targeturl_query'] = url_parsed_target.apply(lambda x: x.query)\n",
    "    test['targeturl_path'] = url_parsed_target.apply(lambda x: x.path)\n",
    "    \n",
    "    s = test.groupby('userid').apply(identify_outliers).drop(['userid', 'level_1'], axis=1).reset_index()\n",
    "    \n",
    "    del test\n",
    "\n",
    "    s_v = s.groupby('sessionid').subsessionid.last()\n",
    "    s_t = s[s.subsessionid.isin(s_v)]\n",
    "    s_t_list = s_t.groupby('subsessionid').apply(get_grouped_subsessionid_list)\n",
    "    s_t_list_netloc = s_t.groupby('subsessionid').apply(\n",
    "        get_grouped_subsessionid_list, \n",
    "        col1='referrerurl_netloc',\n",
    "        col2='targeturl_netloc'\n",
    "    )\n",
    "    st_grouped = s_t.groupby('subsessionid').agg(\n",
    "        subsession_duration=('eventtimestamp', lambda x:np.max(x)-np.min(x)),\n",
    "        platforms_used=('platform', lambda x: x.unique().tolist()),\n",
    "    )\n",
    "    st_grouped['url_link_list'] = s_t_list\n",
    "    st_grouped['urlloc_link_list'] = s_t_list_netloc\n",
    "    st_grouped['reduced_urlloc_link_list'] = st_grouped.urlloc_link_list.apply(lambda x: [v for i, v in enumerate(x) if i == 0 or v != x[i-1]])\n",
    "    return st_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ae0c44-a472-491a-bcc6-4f714faa0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/'\n",
    "files = [os.path.join(data_path, doc) for doc in os.listdir(data_path) if doc.endswith('parquet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37debf68-c8ae-46f1-bf8f-733ce3357b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/data_0.parquet\n",
      "../data/raw/data_1.parquet\n",
      "../data/raw/data_10.parquet\n",
      "../data/raw/data_11.parquet\n",
      "../data/raw/data_12.parquet\n",
      "../data/raw/data_13.parquet\n",
      "../data/raw/data_14.parquet\n",
      "../data/raw/data_15.parquet\n",
      "../data/raw/data_16.parquet\n",
      "../data/raw/data_17.parquet\n",
      "../data/raw/data_18.parquet\n",
      "../data/raw/data_19.parquet\n",
      "../data/raw/data_2.parquet\n",
      "../data/raw/data_20.parquet\n",
      "../data/raw/data_21.parquet\n",
      "../data/raw/data_22.parquet\n",
      "../data/raw/data_23.parquet\n",
      "../data/raw/data_24.parquet\n",
      "../data/raw/data_25.parquet\n",
      "../data/raw/data_26.parquet\n",
      "../data/raw/data_27.parquet\n",
      "../data/raw/data_28.parquet\n",
      "../data/raw/data_29.parquet\n",
      "../data/raw/data_3.parquet\n",
      "../data/raw/data_30.parquet\n",
      "../data/raw/data_31.parquet\n",
      "../data/raw/data_32.parquet\n",
      "../data/raw/data_33.parquet\n",
      "../data/raw/data_34.parquet\n",
      "../data/raw/data_35.parquet\n",
      "../data/raw/data_36.parquet\n",
      "../data/raw/data_37.parquet\n",
      "../data/raw/data_38.parquet\n",
      "../data/raw/data_39.parquet\n",
      "../data/raw/data_4.parquet\n",
      "../data/raw/data_40.parquet\n",
      "../data/raw/data_41.parquet\n",
      "../data/raw/data_42.parquet\n",
      "../data/raw/data_43.parquet\n",
      "../data/raw/data_44.parquet\n",
      "../data/raw/data_45.parquet\n",
      "../data/raw/data_46.parquet\n",
      "../data/raw/data_47.parquet\n",
      "../data/raw/data_5.parquet\n",
      "../data/raw/data_6.parquet\n",
      "../data/raw/data_7.parquet\n",
      "../data/raw/data_8.parquet\n",
      "../data/raw/data_9.parquet\n"
     ]
    }
   ],
   "source": [
    "file_info = []\n",
    "for file in files: \n",
    "    #info = pd.read_parquet(file)\n",
    "    info = process_file(file)\n",
    "    file_info.append(info)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19cba07-090d-4672-a574-d2fbbb0bcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.concat(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af64b3bf-0ec6-41c1-99db-b945e6d2f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/data_0.parquet\n",
      "../data/raw/data_1.parquet\n",
      "../data/raw/data_10.parquet\n",
      "../data/raw/data_11.parquet\n",
      "../data/raw/data_12.parquet\n",
      "../data/raw/data_13.parquet\n",
      "../data/raw/data_14.parquet\n",
      "../data/raw/data_15.parquet\n",
      "../data/raw/data_16.parquet\n",
      "../data/raw/data_17.parquet\n",
      "../data/raw/data_18.parquet\n",
      "../data/raw/data_19.parquet\n",
      "../data/raw/data_2.parquet\n",
      "../data/raw/data_20.parquet\n",
      "../data/raw/data_21.parquet\n",
      "../data/raw/data_22.parquet\n",
      "../data/raw/data_23.parquet\n",
      "../data/raw/data_24.parquet\n",
      "../data/raw/data_25.parquet\n",
      "../data/raw/data_26.parquet\n",
      "../data/raw/data_27.parquet\n",
      "../data/raw/data_28.parquet\n",
      "../data/raw/data_29.parquet\n",
      "../data/raw/data_3.parquet\n",
      "../data/raw/data_30.parquet\n",
      "../data/raw/data_31.parquet\n",
      "../data/raw/data_32.parquet\n",
      "../data/raw/data_33.parquet\n",
      "../data/raw/data_34.parquet\n",
      "../data/raw/data_35.parquet\n",
      "../data/raw/data_36.parquet\n",
      "../data/raw/data_37.parquet\n",
      "../data/raw/data_38.parquet\n",
      "../data/raw/data_39.parquet\n",
      "../data/raw/data_4.parquet\n",
      "../data/raw/data_40.parquet\n",
      "../data/raw/data_41.parquet\n",
      "../data/raw/data_42.parquet\n",
      "../data/raw/data_43.parquet\n",
      "../data/raw/data_44.parquet\n",
      "../data/raw/data_45.parquet\n",
      "../data/raw/data_46.parquet\n",
      "../data/raw/data_47.parquet\n",
      "../data/raw/data_5.parquet\n",
      "../data/raw/data_6.parquet\n",
      "../data/raw/data_7.parquet\n",
      "../data/raw/data_8.parquet\n",
      "../data/raw/data_9.parquet\n"
     ]
    }
   ],
   "source": [
    "for f, inf  in zip(files, file_info):\n",
    "    print(f)\n",
    "    inf.to_parquet(f.replace('raw','interim'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "561d72ec-eaf7-4153-a7d1-41bd19c64ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ef27e-d7bf-49c8-a53e-8cd2624e927c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
